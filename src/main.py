"""
Main application entry point for the news aggregation system.
"""
import time
from typing import List
from models import Article, NewsCategory
from data import generate_sample_articles, get_news_sources
from filters import NewsFilter
from compiler import MagazineCompiler
from utils import (
    simulate_api_fetch, setup_logging, print_processing_step,
    print_statistics, print_banner, save_to_file
)

def simulate_data_collection() -> List[Article]:
    """Simulate collecting articles from various news sources."""
    print_processing_step("SIMULATING DATA COLLECTION", "Fetching from 10 news sources...")
    
    articles = generate_sample_articles()
    sources = get_news_sources()
    
    # Group articles by source for realistic simulation
    articles_by_source = {}
    for article in articles:
        if article.source not in articles_by_source:
            articles_by_source[article.source] = []
        articles_by_source[article.source].append(article)
    
    # Simulate fetching from each source
    all_articles = []
    for source in sources:
        if source.name in articles_by_source:
            source_articles = articles_by_source[source.name]
            simulate_api_fetch(source.name, len(source_articles))
            all_articles.extend(source_articles)
            source.articles_fetched = len(source_articles)
        else:
            simulate_api_fetch(source.name, 0)
    
    print(f"\n‚úÖ Data collection complete: {len(all_articles)} articles retrieved")
    return all_articles

def demonstrate_filtering_system():
    """Demonstrate the complete news aggregation and filtering system."""
    print_banner()
    setup_logging('INFO')
    
    # Step 1: Simulate data collection
    articles = simulate_data_collection()
    time.sleep(1)
    
    # Step 2: Initialize filter system
    print_processing_step("INITIALIZING FILTER SYSTEM", "Setting up relevance scoring and duplicate detection...")
    news_filter = NewsFilter()
    time.sleep(0.5)
    
    # Step 3: Process articles
    print_processing_step("PROCESSING ARTICLES", "Calculating relevance scores and analyzing sentiment...")
    processed_articles = news_filter.process_articles(
        articles,
        remove_duplicates=True
    )
    
    duplicate_count = len(articles) - len(processed_articles)
    print_statistics(processed_articles, duplicate_count)
    time.sleep(1)
    
    # Step 4: Compile magazine
    print_processing_step("COMPILING MAGAZINE", "Organizing articles into sections and generating summaries...")
    compiler = MagazineCompiler()
    magazine = compiler.compile_magazine(processed_articles)
    time.sleep(0.5)
    
    # Step 5: Display results
    print_processing_step("GENERATING OUTPUT", "Creating consumer-ready news digest...")
    
    # Console output
    console_output = compiler.export_magazine(magazine, 'console')
    print(console_output)
    
    # Save HTML version
    html_output = compiler.export_magazine(magazine, 'html')
    save_to_file(html_output, 'news_digest.html')
    
    # Final summary
    print("\nüéâ NEWS AGGREGATION COMPLETE!")
    print(f"   üìù Magazine compiled with {len(magazine['sections'])} sections")
    print(f"   üìä Total articles included: {sum(section['article_count'] for section in magazine['sections'].values())}")
    print(f"   üíæ HTML version saved as 'news_digest.html'")
    
    return magazine, processed_articles

def demonstrate_category_filtering():
    """Demonstrate category-specific filtering."""
    print("\n" + "="*60)
    print("üîç CATEGORY FILTERING DEMONSTRATION")
    print("="*60)
    
    articles = generate_sample_articles()
    news_filter = NewsFilter()
    
    # Filter for finance news only
    print("\nüìà FINANCE NEWS FILTER")
    finance_articles = news_filter.process_articles(
        articles,
        category_filter=[NewsCategory.FINANCE]
    )
    
    print(f"Found {len(finance_articles)} finance articles:")
    for article in finance_articles[:3]:  # Show top 3
        print(f"  ‚Ä¢ {article.title} (Score: {article.relevance_score:.2f})")
    
    # Filter for technology news
    print("\nüíª TECHNOLOGY NEWS FILTER")
    tech_articles = news_filter.process_articles(
        articles,
        category_filter=[NewsCategory.TECHNOLOGY]
    )
    
    print(f"Found {len(tech_articles)} technology articles:")
    for article in tech_articles[:3]:  # Show top 3
        print(f"  ‚Ä¢ {article.title} (Score: {article.relevance_score:.2f})")

def demonstrate_keyword_filtering():
    """Demonstrate keyword-based filtering."""
    print("\n" + "="*60)
    print("üîë KEYWORD FILTERING DEMONSTRATION")
    print("="*60)
    
    articles = generate_sample_articles()
    news_filter = NewsFilter()
    
    # Filter by AI-related keywords
    ai_keywords = ['artificial intelligence', 'AI', 'machine learning', 'automation']
    print(f"\nü§ñ Filtering for AI-related news: {ai_keywords}")
    
    ai_articles = news_filter.process_articles(
        articles,
        keyword_filter=ai_keywords
    )
    
    print(f"Found {len(ai_articles)} AI-related articles:")
    for article in ai_articles:
        print(f"  ‚Ä¢ {article.title} (Score: {article.relevance_score:.2f}, Source: {article.source})")

if __name__ == "__main__":
    # Run the main demonstration
    magazine, processed_articles = demonstrate_filtering_system()
    
    # Run additional demonstrations
    demonstrate_category_filtering()
    demonstrate_keyword_filtering()
    
    print("\n" + "="*80)
    print("üìö SYSTEM EXTENSION GUIDE")
    print("="*80)
    print("""
To extend this system for real web scraping and API integration:

1. üåê Replace simulate_api_fetch() with actual HTTP requests
   - Use requests library for web scraping
   - Implement API clients for news sources (NewsAPI, Guardian API, etc.)
   - Add rate limiting and error handling

2. üóÑÔ∏è Add database integration
   - Store articles in PostgreSQL/MongoDB
   - Implement caching to avoid duplicate fetching
   - Add article archival and search capabilities

3. üöÄ Implement real-time processing
   - Use message queues (Redis, RabbitMQ) for article processing
   - Add scheduled tasks for periodic fetching
   - Implement incremental updates

4. üîß Enhance NLP capabilities
   - Use spaCy or NLTK for advanced text analysis
   - Implement named entity recognition
   - Add topic modeling and clustering

5. üìä Add analytics and monitoring
   - Track processing metrics
   - Monitor source reliability
   - Add performance dashboards

6. üåç Scale for production
   - Containerize with Docker
   - Deploy to cloud platforms
   - Add load balancing and auto-scaling
""")
    
    print("üéØ System demonstration completed successfully!")